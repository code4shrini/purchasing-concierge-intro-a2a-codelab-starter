# .env

# --- ChromaDB Configuration (for mcp_chroma_server.py) ---
# Choose one:
# CHROMA_CLIENT_TYPE=ephemeral
#CHROMA_CLIENT_TYPE=persistent
#CHROMA_DATA_DIR=./chroma_data_storage # Directory for persistent ChromaDB data

# If using http client, configure the host and port where your ChromaDB server runs:
 CHROMA_CLIENT_TYPE=http
 CHROMA_HOST=localhost
 CHROMA_PORT=8000

# --- LLM Configuration (for adk_agent.py) ---
# Example for OpenAI
# OPENAI_API_KEY="YOUR_OPENAI_API_KEY"

# Example for Ollama (e.g., running Gemma locally)
#OLLAMA_API_BASE="http://localhost:11434"
 LITELLM_MODEL="gemini/gemini-1.5-flash" #"ollama/gemma-3:12b" # This might be used by LiteLlm internally, depending on your LiteLlm setup